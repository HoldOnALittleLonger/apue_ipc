UNIX Network Programming Volume 2
 - Interprocess Communications

Chapter 1: Overview
    IPC (interprocess communication):
      It means the methods for message passing between different processes in a system.

    Synchronize (sync):
      A way to guarantee a order between different processes access shared resource concurrently.

    IPC in the book to be discussed:
      <IPC type>                   <persistent>		 <namespace, identifier>
      pipe                         (process-persistent)  (noname, descriptor)
      fifo                         (process-persistent)  (path, descriptor)
      SystemV IPC:
        message queue              (kernel-persistent)   (key_t, SystemV IPC ID)
	semaphore                  (kernel-persistent)   (key_t, SystemV IPC ID)
	shared memory              (kernel-persistent)	 (key_t, SystemV IPC ID)
      Posix IPC:
        message queue              (kernel-persistent)   (Posix IPC name, mqd_t)
	semaphore                  (kernel-persistent)   (Posix IPC name, sem_t)
	unnamed semaphore          (process-persistent)  (noname, sem_t)
	shared memory              (kernel-persistent)	 (Posix IPC name, descriptor)
	mutex                      (process-persistent)  (noname, pthread_mutex_t)
	condition variable         (process-persistent)  (noname, pthread_cond_t)
	(fcntl) record lock        (process-persistent)  (path, descriptor)
	read-write lock            (process-persistent)  (noname, pthread_rwlock_t)
      RPC (Remote Procedure Call)  (by achieving)        (program/version, RPC ID)
      Door	  	    	   (by achieving)	 (path, descriptor)
      Network socket               (process-persistent)  (IP + TCP OR UDP | path, descriptor)

    Evolution of Synchronizing in UNIX:
      a way to sync between processes for write or read a file ->
      record locking ->
      SystemV semaphore ->
      Posix semaphore ->
      mutex ->
      condition variable ->
      read-write lock (it is noe contained in POSIX.1)

    Shared message for process and thread:
      Three method to shared message between processes:
        1>  through filesystem
	2>  through kernel 
	3>  through shared memory

      #  almost all of IPC can used between threads,because in Linux,thread just a task which
         has some process address space and shared process resource.

    Persistence of IPC object:
      persistenct of any type for IPC can be defined as time how long the object of a IPC is existed.
      Three type persistence:
        1>  process-persistent    -
	      exist until the last process exit which holding the IPC object.
	2>  kernel-persistent     -
	      exist until the kernel reboot or delete it in explicitly.
	3>  filesystem-persistent -
	      exist until delete the IPC object explicitly.
	      (because the IPC object exist in filesystem,it will be existing even reboot)

	#  Posix.1 allows to achieve Posix_semaphore Posix_message_queue Posix_shared_memory in
	   different ways.(filesystem or kernel object all is allowed)

    Name Space:
      Name space is a set of available names of IPC for especial type.
      IPC object must has a name or a identifier to identify it.No related processes can access the IPC
      through the identifier.(pipe just such a IPC between related processes)

      #  related processes:
           processes has same parent process of has same grandparent process.
	   in normally,almost all user process has same grandparent process it is a login shell.
      #  Generic standard Name Space is working by Posix.1g workgroup.

    Global variable for error:
      errno  --  <sys/errno.h>
        system call interfaces would use this variable errno to indicates which type of error had happened.
	(IPC interfaces is alike in same behavior)

      #  No any error equal to 0,all error macro definition is begin with 'E'

      Macro for thread environment:
        _REENTRANT                /* can reentry */
	_POSIX_C_SOURCE 199506L   /* POSIX.1 standard 199506L */

	either macro definition would effective thread special errno variable.
	in this case,errno maybe expand to a function and program can use this function to access thread 
	special errno copy.
         
    Standard:
      POSIX: (Poartable Operating System Interface) <ISO/IEC 9945>
        POSIX.1  -  System application program interface(API)[C Language]  (IEEE Std 1003.1-1988)
        POSIX.2  -  Shell and utilities	       				   (IEEE Std 1003.2-1992)
        POSIX.3  -  System administration  (continious working)

	#  Socket and XTI were contained in IEEE Std 1003.1 -- P1003.1g

      X/OPEN:
        XPG  -  X/OPEN Portablity Guide
	SUS  -  Single UNIX standard


Chapter 2 : POSIX IPC
    All posix ipc is contained in REALTIME standard,so if program used them in code,then have to link
    rt library.(-lrt)

    POSIX IPC:
      Message Queue: (detail in Manual) <mqueue.h>
        Service Primitive:
	  mq_open
	  mq_close
	  mq_unlink
	  mq_getattr
	  mq_setattr
	  mq_send
	  mq_receive
	  mq_notify

	struct mq_attr {
	       long mq_flags;
	       long mq_maxmsg;
	       long mq_msgsize;
	       long mq_curmsgs;
	};
      Semaphore: (detail in Manual) <semaphore.h>
        Service Primitive:
	  sem_open
	  sem_close
	  sem_unlink
	  sem_init
	  sem_destroy
	  sem_wait
	  sem_trywait
	  sem_post
	  sem_getvalue
      
      #  posix semaphore is contained in thread standard,so have to link pthread library.(-lpthread)

      Shared Memory: (detail in Manual) <sys/mman.h>
        Service Primitive:
	  shm_open
	  shm_unlink
	  
	  /* these function is unix standard */
	  /* posix shared memory will be abstracted as a file descriptor */
	  ftruncate
	  fstat
	  mmap
	  munmap

    POSIX IPC Name:
      rules of posix ipc name:
        1>  Length of name cant exceed PATH_MAX(contains '\0')
	2>  different between full path and relative path depend on implementation
	3>  interpretation for additional '/' depend on implementation

      additional rules:
        1>  must start with '/'
        2>  only one '/' can appears in posix ipc name

	#  this rule for improve portiablity but not all implementation support.
	   realtime standard attempts to allow achieve POSIX IPC in kernel,the 
	   gain is that posix ipc can works fine on non-disk machine,all resource
	   create in memory.for that,some architecture cant support this.
	   (A standard way of being nonstandard - POSIX)

    POSIX IPC type checking macro:
      bool S_TYPEISMQ(buf);
      bool S_TYPEISSEM(buf);
      bool S_TYPEISSHM(buf);

      buf is a struct type of stat defined in <sys/stat.h>
      in usually,these macro should be inefficiency because of it that they
      depend on type of IPC how to implement.
      if POSIX IPC doesnt implement in a especial file type,then these macro
      always return false.

    POSIX IPC open and create:
      Permission allowed of IPC to be open:
        MQ:
	  O_RDONLY | O_WRONLY | O_RDWR
	SEM:
	  O_RDWR  /* operate a semaphore needs RDWR permission */
	SHM:
	  O_RDONLY | O_RDWR

      Create of IPC:
        MQ AND SEM AND SHM:
	   if not existed,then try to create it.
	   
	   O_CREAT | O_EXCL
	   /* if it existed but assigned O_CREATE | O_EXCL,then return -1,errno = EEXIST */

      NONBlock open:  /* O_NONBLOCK */
        MQ

      Truncate:
        /* if it existed,allow to trunc it */
	SHM

      User-ID and Group-ID:
        MQ:
	  user-id = process effective user-id
	  group-id = process effective group-id 

	SEM AND SHM:
	  user-id = process effective user-id
	  group-id = process effective group-id OR a effective system default group-id
	  /* open() a file,the group id could be process group-id or directory group-id */

      Permission checking when open or create a POSIX IPC:
        point to be depened for checking:
	  1>  permission from create IPC
	  2>  permission from accessing
	  3>  user-id and group-id

	order for checking:
	  if (process user-id == 0)
	    allow();
	  else if (user-id == process user-id && permission == accessing permission)
	    allow();
	  else if ((group-id == process group-id || group-id == process attached group-id)
	       	  && permission == accessing permission)
	    allow();
	  else if (other-user-permission == accessing permission)
	    allow();
	  else
	    refuse();

      Figure for open POSIX IPC:
        open -> if existed {
	     	  if O_CREAT | O_EXCL is true
		    open failed
		  else if permission approve
		    open successed  /* this would open a POSIX IPC which is existed */
		  else
		    open failed
	     	}
		else {
		  if O_CREAT is true
		     if resource avaiable
		       open successed  /* this would create a POSIX IPC */
		     else
		       open failed
		  else
		    open failed
		}

Chapter 3 : SystemV IPC
    SystemV IPC is not supported by Posix.2,it was contained in Unix98.

    SystemV IPC:
      Message Queue: <sys/msg.h>
        Service Primitive:
	  msgget
	  msgctl
	  msgsnd
	  msgrcv

      Semaphore: <sys/sem.h>
        Service Primitive:
	  semget
	  semctl
	  semop

      Shared Memory: <sys/shm.h>
        Service Primitive:
	  shmget
	  shmctl
	  shmat
	  shmdt
        
    SystemV IPC Name:
      SystemV IPC use a value of key_t as key name,this value is combined by path and id via ftok().

      <sys/types.h>
      <sys/ipc.h>
      key_t ftok(const char *pathname, int proj_id);
      >>  in normally,ftok() would take stat.st_dev and stat.st_ino of pathname,and combines these value with the
          low 8bits of proj_id,then return result as key_t.
	  (if proj_id == 0,then behavior of ftok() is undefined,but in glibc,no such request)

      #  Special key for SystemV IPC is IPC_PRIVATE(equal to 0 normally),
         XXXget() can use this key to create a unique IPC object.but this IPC object should not used by
	 processes they are not relative.

    SystemV IPC struct ipc_perm:
      kernel maintains a struct ipc_perm for each IPC object.

      <sys/ipc.h>
      struct ipc_perm {
        key_t __key;		/*  key_t  */
	uid_t uid;		/*  owner uid  */
	gid_t gid;		/*  owner gid  */
	uid_t cuid;		/*  creator uid  */
	gid_t cgid;		/*  creator gid  */
	unsigned short mode;	/*  permissons  */
	unsigned short __seq;  	/*  sequence number  */
      };

      XXXget() function would use some bits in flag argument to set ipc_perm.mode.
      __key == key argument.
      uid == caller uid.
      gid == caller gid.
      cuid == creator uid.
      cgid == creator gid.
      mode almost like mode in open()(linux),but in UNIX has some difference.
      >>  MSG_R	       (owner permission)
      	  MSG_W
	  MSG_R >> 3   (group permission)
	  MSG_W >> 3
	  MSG_R >> 6   (other permission)
	  MSG_W >> 6
	  SEM_R
	  SEM_A(SEM_W)
	  SEM_R >> 3
	  SEM_A >> 3
	  SEM_R >> 6
	  SEM_A >> 6
	  SHM_R
	  SHM_W
	  SHM_R >> 3
	  SHM_W >> 3
	  SHM_R >> 6
	  SHM_W >> 6
      special flag for XXXget(),IPC_CREATE,IPC_EXCL alike to O_CREATE O_EXCL.
      /*  different IPC has different ipc_perm.mode(detail in manual)  */

      __seq like a refer counter,every IPC object was delete,its value would increase,
      if exceed range then begin from 0.
      >>  reasons of __seq:
      	    1> prevent process scan global IPC object try to does some badly action.
	       everytime to reuse a IPC table item,the identifier returned by XXXget()
	       would add IPC table item number.
	    2> prevent reuse SystemV IPC identifier in short time.

      /*  __seq just increase while creating a IPC via same key,then identifier would be changed  */
      /*  seq: 0 -> 1 -> 2 -> ...  identifier: 0 -> 32768 -> 65536 -> ...  */
      /*  this behavior would be reset until kernel reboot  */

      #  XXXctl() can changes owner ID via IPC_SET command,but cant change creator ID.
      #  ipcs program can report system IPC state,ipcrm program can delete IPC(XXXctl() use IPC_RMID command).

    IPC permission checking:
      first checking:
        XXXget(),if flag open some bits they are closed in ipc_perm.mode.(get a existed IPC)
      second checking:
        check permissions everytime to access IPC object.
	a> uid == 0 OK
	b> uid == ipc_perm.uid OR uid == ipc_perm.cuid AND mode OK
	c> gid == ipc_perm.gid OR gid == ipc_perm.cgid AND mode OK
	d> other,mode(other accessible) OK

    Limit on IPC:
      kernel limit SystemV IPC,but there has a way to change limit via system config parameter(e.g. sysctl)
      default limit range is very small.


Chapter 4 : Messaging between processes(PIPE, FIFO)
    pipe : supports messaging between parent process and child process.
    fifo : supports messaging between no relative processes.

    #  generally,pipe is half-duplex,but some system supports full-duplex pipe(SVR4).
    #  socketpair() function supports full-duplex,it was contained in SVR4.
    #  POSIX.1 and UNIX98 just required half-duplex pipe.
    #  socket is full-duplex(even unix domain socket).

    PIPE :
      <unistd.h>
      int pipe(int fd[2]);
      #  return 0 if succeed,return -1 if fault.
      >>  fd[0] as read,fd[1] as write.

      <stdio.h>
      FILE *popen(const char *command, const char *type);
      #  return FILE pointer if succeed,return NULL if fault.
      >>  command would be executed by shell,type tell shell how to connect this pipe between caller.
      	  type == "r" means caller want read data from command,
	  type == "w" means caller want write data to command.

      int pclose(FILE *stream);
      #  return terminate state of shell if succeed,return -1 if failed.
      >>  close a stream open by popen().

    FIFO : 
      <sys/stat.h>
      <sys/types.h>
      <fcntl.h>
      <unistd.h>
      int mkfifo(const char *pathname, mode_t mode);
      #  return 0 if succeed,return -1 if failed.
      >>  mkfifo() try to creates a named pipe in file-system with the mode.
      	  if such pipe has been existed,mkfifo() will failed.
	  for use the fifo,process have to call open() to open it,
	  use read() or write() to operates it,
	  use close() to close fd,use unlink() to delete the fifo.

    Attributes of pipe and fifo :
      flag O_NONBLOCK set up for pipe and fifo :
        pipe : use fcntl() to set it.
	fifo : specify O_NONBLOCK when call open().

      actions when operates pipe or fifo :
        open fifo with RD, a process open fifo with WR already,
	  return(block), return(nonblock)
	open fifo with RD, no process open fifo with WR already, 
	  block until a process open fifo with WR(block), return(nonblock)
	open fifo with WR, a process open fifo with RD already, 
	  return(block), return(nonblock)
	open fifo with WR, no process open fifo with RD already,
	  block until a process open fifo with RD(block), return ENXIO(nonblock)

	read empty fifo or pipe, pipe or fifo has been opened with WR, 
	  block until data come or the pipe or fifo was closed(block), return EAGAIN(nonblock)
	read empty fifo or pipe, pipe or fifo has not been opened with WR, 
	  return 0(EOF)(block), return 0(EOF)(nonblock)
	
	write pipe or fifo, pipe or fifo has not been opened with RD, 
	  deliver SIGPIPE to caller(block), deliver SIGPIPE to caller(nonblock)
	write pipe or fifo, pipe or fifo has been opeend with RD :
	      (block)
	        if size_n <= PIPE_BUFF
		then
		  if space_of_pipe >= size_n
		  then
		    write all data
		  else
		    block until has space to write (return EAGAIN,nonblock)
		else
		  if space_of_pipe >= 1B
		  then
		    write apart of data
		  else
		    block until has space to write (return EAGAIN,nonblock)

      limit : 
        <limits.h>
        PIPE_BUF : a data size limit of atomic pipe operation defined in <limit.h>,
		   generally is 512B,but there is could be changed for FIFO,
		   POSIX.1 consider FIFO is a pathname variable depends on file-system.
		   so pathconf() and fpathconf() can query its value.

		   #  if size_n > PIPE_BUF,then kernel not promise operation is atomic.
		   #  maybe data1 and data2 interleaving.

        OPEN_MAX : the maxinum size of number for opened files in a process.
		   sysconf() can query its value.

    Single sever,several clients model :
      Procedure :
        Sever -> mkfifo -> open it with read and then open it with write
	-> (cycle point) wait for message from client -> open client's fifo 
	-> send required data by client	-> close client's fifo
	-> enter next cycle

	#  sever open its fifo twice that is to prevent open this fifo
	#  everytimes while client came

	Client -> mkfifo -> open sever fifo for write -> send request
	-> open client fifo for read -> wait data from sever -> close
	-> unlink fifo

	#  there is a convention that is FIFO and PIPE just works based
	#  single direction working.

      A properly request format :
        Request format : "<PID NUMBER> <REQUEST>"

      Concurrent server :
        one-child-per-client :
	  1> fork child server to serve client.
	  2> create child thread to serve client.

      DoS :
        server open client's fifo with BLOCKED,but client never open its fifo.
	server would blocked until restart.

    Atomic attribute of FIFO(PIPE) write :
      data size for write must less than or equal to PIPE_BUF,
      OS will promise the datas from several client never interleaving.

    Attentions :
      if close a FIFO or PIPE,the all data in it would be abanded by kernel.
      FIFO cant be used in NFS.

    Byte stream and message :
      Byte stream : no bound to specify message length,this is the basic UNIX I/O model.
      Message : has bound to specify message length,it is alike to datagram.(UDP, Sun RPC)

      Several method to build Message over Byte stream :
        1> special end flag (\n, \r ...)
	#  but if such flag symbol was contained in data,it would be interpreted as end flag.

	2> explicit length (datagram header)
	#  struct header { size_n, type_id, buffer ... };

	3> one record one connection
	#  this method might cause resource wasting.(create new connection ...)



Chapter 15 : Door
    (Solaris 2.6)
    /*  Linux 3.10 rpc function based Sun RPC  */

    door is a ipc for remote procedure call,it is invented for Spring distributed operating system.
    LPC(local procedure call) : the procedure was called and the caller in same process.
    	      		        it is synchronous call.
				(POSIX thread provides a way to do asynchronous procedure call)
    RPC(remote procedure call) : the procedure was called and the caller in different processes.
    	       		       	 it is asynchronous call.
    	       		         (RPC supports different hosts in network)

    local remote procedure call :
      RPC in host scope,the different processes in same host.

    door :
      a process calls the procedure of another process in a same host.(door supports network communication)
      it is the enter point of procedure which in server process,client use it to access the procedure.
      and it is synchronous call.

      caller and called are communicate in arguments and returns for information exchange.
      
      Solaris 2.6 door is implementation is involves to thread.
      one client to one server thread,so the server door procedure must be thread-safe.

      usage order of door in Solaris 2.6 :
        server :
	  door_create ->  create door,door use descriptor to identify
	  fattach ->      relate filesystem path to door ipc
	  enter block ->  wait for client
	  door_return 	  return result to client
	  
    	client :
	  open ->	  open door path in filesystem
	  door_call ->	  call procedure in server
	  wait server return

      because use descriptor to identify door ipc,so could send door to another process via
      descriptor transfer.

      <door.h>
        typedef struct door_arg {
	    char *data_ptr;		
	    size_t data_size;
	    door_desc_t *desc_ptr;
	    size_t desc_num;
	    char *rbuf;
	    size_t rsize;
	} door_arg_t;

	data_ptr  : call-> ptr to data arguments
		    return-> ptr to data results
	data_size : call-> #bytes of data arguments
		    return-> actual #bytes of data results
	desc_ptr  : call-> ptr to descriptor arguments
		    return-> ptr to descriptor results
	desc_num  : call-> number of descriptor arguments
		    return-> number of descriptor results
	rbuf      : ptr to result buffer
	rsize     : #bytes of result buffer

	the format of arguments and return-values,have to take an agreement ahead between client and server.
	for eazy design,could make encapsulation for arguments or results in a structure type.
	no arguments data_ptr have set to NULL,and data_size set to zero.

	desc_ptr is an array of door_desc_t to save descriptors they would transfer to server or
	receive from server.

	rbuf is the buffer,arguments and results can use same buffer,that means data_ptr and desc_ptr could
	point to rbuf,and it is also OK when return.
	if the size of rbuf is too less that cant save results from server,then door library function would
	allocate a new buffer through mmap() in caller process address space automatically,then update rbuf
	and rsize.if a new buffer was allocated,then user have to call munmap() to unmap the buffer.

	int door_call(int fd, door_arg_t *argp);
	/*
	 *  door_call() should be called by client,it would calls a procedure in server process address space.
	 *  @fd  :  a file descriptor which identify the door ipc,normally,it is returned by open() to open
	 *          door ipc file in filesystem.
	 *  @argp  :  structure pointer points to door_arg_t object which contains call arguments and also used
	 *         :  as result buffer to saves results from server procedure call.
	 *  return - return 0 if succeed,otherwise -1.
	 */
	 /*  should always access result via argp.data_ptr,because rbuf might reallocate.  */


	int door_create(void (*server_procedure)(void *cookie, char *argp, size_t arg_size, door_desc_t *dp,
	    		     			 uint_t n_desc), void *cookie, uint_t attributes);
	/*  @dp : dataptr  */
	/*
	 *  door_create() is such function that create door and returns the descriptor.
	 *  @server_procedure : this procedure would as server function when a door_call occurs.
	 *  @cookie : cookie as identifier as usual.this parameter would as argument for invoke server_procedure.
	 *  	      caller could use this argument pass additional message to server_proc.
	 *  @attributes : attributes for newly created door.
	 *  		  Valid value : (could combine with OR)
	 *		  	      DOOR_UNREF
	 *			      DOOR_UNREF_MULTI
	 *			      DOOR_PRIVATE
	 *			      DOOR_REFUSE_DESC
	 *			      DOOR_NO_CANCEL
	 *  return - return descriptor of door if succeed,otherwise returns -1.
	 */
	    DOOR_PRIVATE : indicates that the door has the server threads poor owns to itself.
	    		   when an client request is came,door library function would create
			   server thread automatically,and the thread would put into process
			   threads poor in default.
	    DOOR_UNREF   : when the number of descriptors that refer to this door drops to one,
	    		   recall server_proc once again,by the time, DOOR_UNREF_DATA designates
			   an unreferenced invocation,as the argp argument passed to server_procedure.
			   in the case of an unreferenced invocation,the values for arg_size,dp and
			   n_desc are 0.only one unreferenced invocation is delivered on behalf of a door.
			   ways to refer a door :
			     1>  file descriptor returned by door_create(). 
			     	 (generally,this fd is continously opened until server process stop,this is
				  whay unreferenced invocation condition is drops to one not zero)
			     2>  filesystem path mapping by fattach().	    
			     3>  client open door for door_call().	    
			   ways to decrease door references :
			     1>  close file descriptor returned by door_create().
			     2>  unlink the path mapped by fattach().
			     3>  client close the file descriptor returned by open().
			   /*  generally,call door_return(NULL, 0, NULL, 0) to return from an unreferenced 
			       invocation.  */

	/*  the door descriptor is default exist FD_CLOEXEC.
	 *  process forks a child process,just only parent is able to receive request from client,even it still
	 *  open in child.
	 */

	!!  for make a mapping between filesystem path and door descriptor,have to invoke fattach() to map path to
	    door descriptor.client can only access the door via filesystem path.
	!!  function fdetach() undo such mapping.
	!!  the thread which running a server_proc would be created only an invocation of door_call() by client is 
	    came.if server process has several server procedure,all server_proc would put into same thread poor.
	    

	int door_return(char *dataptr, size_t datasize, door_desc_t *descptr, size_t *ndesc);
	/*
	 *  door_return - return from a door invocation.just can call this function from server_proc.
	 *  @dataptr : pointer points to result of data.
	 *  @datasize : size of result buffer.
	 *  @descptr : pointer points to result of descriptor.
	 *  @ndesc : pointer points to a buffer where saves the size of result of descriptor.
	 *  return - no return if succeed,otherwise return -1.
	 */

	 int door_cred(door_cred_t *cred);
	 /*
	  *  door_cred - an interface for server to retrive user identifier from door_call.
	  *  @cred : a pointer points to an buffer which hold size same as structure door_cred_t.
	  *  return - return 0 if succeed,otherwise return -1.
	  */

	 typedef struct door_cred {
	     uid_t dc_euid;    /*  effective user ID of client  */
	     gid_t dc_egid;    /*  effective group ID of client  */
	     uid_t dc_ruid;    /*  real user ID of client  */
	     gid_t dc_rgid;    /*  real group ID of client  */
	     pid_t dc_pid;     /*  process ID of client  */
	 } door_cred_t;

	 /*  door_cread should be called in server_proc or in a function which is be called by server_proc.  */

	 int door_info(int fd, door_info_t *info);
	 /*
	  *  door_info - retrive server information by client.
	  *  @fd : door descriptor.
	  *  @info : structure pointer points to door_info_t.
	  *  return - return 0 if succeed,otherwise return -1.
	  */

	 typedef struct door_info {
	     pid_t di_target;			/*  server process ID  */
	     door_ptr_t di_proc;		/*  server procedure  */
	     door_ptr_t di_data;		/*  cookie for server procedure,the first parameter in server_proc */
	     door_attr_t di_attributes;		/*  attributes associated with door  */
	     door_id_t id_uniquifier;		/*  unique number  */
	 } door_info_t;

	 /*  attributes DOOR_LOCAL and DOOR_REVOKE are possible include in di_attributes.
	     DOOR_LOCAL : the procedure is a part of current process
	     DOOR_REVOKE : the server proc had been destroyed by server via door_revoke()
	 */

	 /*  every door would has a unique number in system scope while it is create,di_uniquifier would save the number  */
	 
	 !!  in normally,client calls door_info() to retrive server informations,but server proc could calls this function,too.
	     just pass DOOR_QUERY as the first argument.

	 int door_bind(int fd);
	 /*
	  *  door_bind - bind current calling thread to the thread poor which associated with the door descriptor.
	  *  	       	 if it is been,then implicitly release current calling thread from the thread poor.
	  *  @fd : the door descriptor.
	  *  return - return 0 if succeed,otherwise return -1.
	  */

	 int door_unbind(void);
	 /*
	  *  door_unbind - explicitly unbind current calling thread from the thread poor it had been binded.
	  *  return - return 0 if succeed,otherwise return -1.
	  */

	 int door_revoke(int fd);
	 /*
	  *  door_revoke - undo door accessing.
	  *  @fd : the door descriptor which would be undo accessing.
	  *  return - return 0 if succeed,otherwise return -1.
	  *  !!  a door can only be undo by a process which created this door.
	  */

	 /*  if a door accessing instance in progressing while server process invokes door_revoke,it still
	  *  effective until completed.
	  */

	 door feature : send file descriptor to client
	   Linux is able to use socket option and the interfaces sendmsg() and recvmsg() to send file descriptor
	   between server and client.
	   Solaris door API provides similar featrue.

	   typedef struct door_desc {
	       door_attr_t d_attributes;	/*  tag for union  */
	       union {
	           struct {			/*  valid if tag == DOOR_DESCRIPTOR  */
		       int d_descriptor;	/*  descriptor number  */
		       door_id_t d_id;		/*  unique id  */
		   } d_desc;
	       } d_data;
	   } door_desc_t;

	   for send descriptor via door,have to set door_desc_t * parameter points to a target object.
	   that object saves the file descriptor in d_descriptor and d_attributes is equal to DOO_DESCRIPTOR.
	   client use door_call() to send file descriptor to server,and server use dool_return() to send file
	   descriptor to client.

	   note :
	     the file descriptor number has same sense from a process to another one just at the time is
	     before fork or after fork,and before exec or after exec.
	     child inherited fd from parent,and the case in exec is let the file descriptor still opened if 
	     FD_CLOEXEC is off.

	   because door_return() dont return,so the file descriptor cant be closed after door_return(),then those
	   fd would still opened until server stop.
	   for solve this issue,have to let server records the fds it opened,and closes them at appropriate time point.
	   Solaris 2.7 update d_attributes,added DOOR_RELEASE,this attribute indicates that close file after its fd
	   had been sended.(DOOR_DESCRIPTOR | DOOR_RELEASE)

	 void (*)() door_server_create(void (*proc)(door_info_t *));
	 /*
	  *  door_server_create - this function is used to set server creation procedure.
	  *  			  every time a client call came,before a new server_proc was
	  *			  created,the procedure @proc have to be processed first.
	  *			  for manually manage server_proc thread,have to use this
	  *			  interface.
	  *			  procedure @proc only be executed when a new server thread
	  *			  is going to create.
	  *  @proc - the procedure which will be called when a client call came and need to create
	  *  	     a new server thread.
	  *  return - returns a function.
	  */
	   normally,door library function automatically create new server thread,but if used this interface,
	   then @proc have to create new server thread to instead of door library function automatically
	   creation.
	   it is able to control server thread number,stack size,etc.
	   the new thread is created by @proc must be PTHREAD_SCOPE_SYSTEM and has attribute PTHREAD_CREATE_DETACHED.
	   if want to control server threads,could use DOOR_PRIVATE attribute when call door_create(),all new 
	   server procedure will be putted into private thread pool.
	   if used private thread pool,then server thread have to calls door_bind() to bind itself with the 
	   private thread pool which is associated to the door.
	   use door_return(NULL, 0, NULL, 0) in server threads let themselves available for incoming door invocations.

	   e.g.
	     void my_create(door_info_t *pDinfo)  /*  procedure for create server thread  */
	     {
	         pthread_t tid;
		 pthread_attr_t attr;
		 pthread_attr_init(&attr);
		 pthread_attr_setscope(&attr, PTHREAD_SCOPE_SYSTEM);
		 pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED);
		 pthread_create(&tid, &attr, my_thread, (void*)pDinfo);
		 pthread_attr_destroy(&attr);
	     }

	     void my_thread(door_info_t *pDinfo)  /*  server thread  */
	     {
	         pthread_setcancelstate(PTHREAD_CANCEL_DISABLE, NULL);
		 door_return(NULL, 0, NULL, 0);
	     }

	     int main(void)
	     {
	         ...
		 (void)door_server_create(my_create);
	     }

	   POSIX thread is allow to cancel default,but thread itself or creator could set cancel state.
	   if client stoped a door call on the server thread is disabled cancel,the door server procedure
	   is still executes but the result from door_return() will be discared.
	   if server procedure allows client stop door call in beforehand,then the server thread which 
	   server procedure on have to deal with this event.

	   kernel thread :
	     kernel thread is the basic lightweight object which can be scheduled indepently and executed
	     on a system processor.kernel thread dont associated with any user process.
	   lightweight process : (hot thread)
	     lightweight process is based on kernel thread but is user visible in user process.
	     every lightweight process is associated with a kernel thread.
	   user thread : (cold thread)
	     thread in user process and for kernel is unvisible.they dont associate with any
	     kernel thread.they are created by thread library function.

	   the threads in a process has two type :
	     1>  thread which bind with a lightweight process.
	     2>  thread which in common lightweight process pool but dont bind with any lightweight process.

	     PTHREAD_SCOPE_SYSTEM means lightweight process binding.
	     PTHREAD_SCOPE_PROCESS means lightweight process unbinding.

    prematurely terminate :
        if server prematurely terminated,but client was blocked for wait result,
        door_call() will be interrupted,system call interrupted.(EINTR)

	system call door_call() is not possible to restart if it was interrupted,
	in the time client calls door_call() and blocked,any signal was received will interrupts door_call().
	return error EINTR.

	idempotent :
	  in the sense for a procedure that is this procedure is able to be called more than once but no any
	  error will occurs.
	non-idempotent :
	  in the sense for a procedure,that is,if it was called more than once,every calls has different result.

	client prematurely terminated >
	  if client was blocked in door_call() but it prematurely terminated,then system send a cancel request
	  to server thread.
	  if server thread wants to care about such request,then it have to does these works :
	    enable cancel state  (default is disabled cancel)
	    if necessary,push cleanup function for thread

	    e.g.
	      void server_proc(void *cookie, char *dataptr, size_t datasize,
	      	   	       door_desc_t *descptr, size_t ndesc)
	      {
	          ...  /*  do somethings  */

		  pthread_setcancelstate(PTHREAD_CANCEL_ENABLE, &oldstate);
		  pthread_cleanup_push(server_proc_cleanup, &cleanup_arg);

		  ...  /*  do somethings  */

		  pthread_cleanup_pop(0);
		  pthread_setcancelstate(oldstate, NULL);

		  ...  /*  do somethings  */

		  door_return((char *)&result, sizeof(result), NULL, 0);
	      }

	      void server_proc_cleanup(void *arg)
	      {
	          ...
	      }

	    /*  thread cleanup mechanism only active at the code between pthread_cleanup_push() and pthread_cleanup_pop()  */


Chapter 16 : Sun RPC
    Sun RPC is the default implementation of RPC in linux.

    Overview :
      distributive application :
        distribute functionality into some processes and these processes use IPC to communicate with each other.

      explicit network programming :
        call socket API or XTI API directly at the program communication part.

      implicit network programming :
        use RPC API to instead of socket API or XTI API.

      RPC API may use network I/O,but for programmer,that should is not visible.
      if an application contains RPC components,then should introduce these components in a RPC specification file,which
      is suffix .x  .
        e.g.
	  struct square_in {
	      long arg1;
	  };

	  struct square_out {
	      long res1;
	  };

	  program SQUARE_PROG {
	      version SQUARE_VERS {
	          square_out SQUAREPROG(square_in) = 1;	/*  procedure number = 1  */
	      } = 1;	     /*  version number  */
	  } = 0x31230000;    /*  program number  */

	  /*  ---  square.x  ---  */
	  /*  Sun RPC software packet tool 'rpcgen',it is able to generate a header from *.x RPC specification file  */
	  /*  for above exampe,rpcgen will generates square_svc. square_xdr.c square_clnt.c square.h  */

      if user want use Sun RPC,then should define a *.x file and use rpcgen to generates these files.
      client program needs *_clnt.c(client stub) *_xdr.c(XDR) and others.
      server program needs *_svc.c(server main file) *_xdr.c and others.
      user have to define main file and others for client program,and have to define server procedure file and others
      for server program.
      server procedure has prototype 
        <return pointer type> <lowercase name_in_*.x>_<version number>_svc(<args pointer type>, struct svc_req *rqstp);
	/*  square_out *squareprog_1_svc(square_in *argp, struct svc_req *rqstp);  */
      and client must call server procedure through client stub,which has prototype 
        <return pointer type> <lowercase name_in_*.x>_<version number>(<args pointer type>, CLIENT *clnt);
	/*  square_out *squareprog_1(square_in *argp, CLIENT *clnt);  */

      /*  !!  before start server,have to ensure 'rpcbind' have been starting,if restart 'rpcbind',all rpc server program
      	      have to restart.
	      rpcbind is a universal address to RPC program number mapper.
	      server register itself on rpc port mapper,client establish connection with rpc port mapper via clnt_create().
      */

      <rpc/rpc.h>
      CLIENT *clnt_create(char *host, unsigned long prog, unsigned long vers, char *proto);
      /*  clnt_create - create client handle for client program.
       *  @host - host name,or IPv4/IPv6 address.
       *  @prog - program name,it should be mapped to program number by rpcgen.
       *  @vers - version number.
       *  @proto - protocol,i.e. tcp,udp, ... .
       *  return - return CLIENT struct pointer,return NULL if failed.
       */

       client - server RPC steps :
         server start
	 server stub blocked until client request has came
	 client calls client stub
	 clnt packs the arguments given by client (marshaling) to construct network message
	 rpc library function invocation,send network message to server(via system call)
	 server received network message
	 server stub unmarshaling arguments from network message
	 server stub calls server procedure with the arguments
	 server procedure working
	 server procedure return result to server stub
	 server stub marshaling result and construct network message
	 rpc library function invocation,send network message to client(via system call)
	 client stub received network message
	 client stub unmarshaling result from network message
	 client stub returns result to client

	 /*  network communication is implicit  */


      /*  generally,clnt and clnt function declare result as static object,that means it is not thread-safe function  */

    Multi-threads :
      in default,Sun RPC server procedure is single-thread,iterative server.
      for on Multi-threads,have to use option on rpcgen.
      -M : start multi-threads
      -A : start a new thread when a request come  /*  linux rpcgen doesnt have this option  */

      if multi-threads is on,then clnt dont use static object to save result,and clnt dont
      returns pointer as longer,it will returns state code,RPC_SUCCESS means rpc call was succeed.
      clnt has new prototype :
        enum clnt_stat <lowercase name_in_*.x>_<version number>(in *argp, out *clnt_res, CLIENT *clnt);
      svc has new prototype :
        bool_t <lowercase name_in_*.x>_<version number>_svc(in *, out *, struct svc_req *);

      in the MT case,user have to define free function :
        int <lowercase name_in_*.x>_<version number>_freeresult(SVCXPRT *transp, xdrproc_t xdr_result, caddr_t result);
	/*  this function just calls "xdr_free(xdr_result, result);" and return 1.
	 *  if server procedure allocated heap space,then this function will recycle memory.
	 *   even dont allocated any memory,it still needed.
	 */

      !!  not all system supports to MT.

    Server binding :
      Sun RPC rpcbind service default running on TCP 111 and UDP 111.before client establish connection with
      rpc server,it have to ask the temporary port of rpc server from rpcbind server,of course,rpc server 
      must register itself on rpcbind.

      the older name of rpcbind is portmap.

      the svc code generated by rpcgen,function svctcp_create() and function svcudp_create() was called,they
      are used to create svc for rpc server with Transport Protocol(default TCP and UDP).for register svc on
      rpcbind,server have to does remote procedure call to rpcbind via svc_register().

      on Unix system,rpc server program can be started by inetd service,but on Linux,systemd had been insteaded
      inetd,just run rpcbind as well(required super user permission).

      <rpc/rpc.h>
      SVCXPRT *svcudp_create(int sock);
      SVCXPRT *svctcp_create(int sock, unsigned int send_buf_size, unsigned int recv_buf_size);
      /*  svcudp_create - create udp svc.
       *  svctcp_create - create tcp svc.
       *  @sock : socket file descriptor,if it is RPC_ANYSOCK,then function
       *  	  opens a new socket.
       *  
       *  @send_buf_size : size of buffer used to send message.0 means default.
       *  @recv_buf_size : size of buffer used to receive message.0 means default.
       *  return - transport pointer structure pointer,failed return NULL.
       */

      bool_t svc_register(SVCXPRT *xprt, unsigned long prognum, unsigned long versnum,
      	     		  void (*dispatch)(svc_req *, SVCPRT *), unsigned long protocol);
      /*  svc_register - register server on portmapper
       *  @xprt : rpc transport pointer.
       *  @prognum : program number.
       *  @versnum : version number.
       *  @dispatch : server dispatch procedure,this procedure is generated by rpcgen in default.
       *  @protocl : transport protocol value,0 means dont register server on portmapper,
       *	     generally,it is zero,IPPROTO_TCP or IPPROTO_UDP.
       *	     if @protocol is nonzero,then a mapping of the triple [prognum,versnum,protocol]
       *	     to xprt->xp_port,established with the portmapper.
       *  return - return true if succeed,otherwise return false.
       */

    Identify :
      in default,RPC request dont contains client identifier,and server dont care about who is current client.
      (null authentication or AUTH_NONE)

      Unix authentication (AUTH_SYS) :
        Unix authetication is made up by host,effect userID,effect groupID,several aux groupID.

	<rpc/svc.h>
	struct svc_req {
	    rpcprog_t rq_prog;		/*  service program number  */
	    rpcvers_t rq_vers;		/*  service protocol version */
	    rpcproc_t rq_proc;		/*  the desired procedure  */
	    struct opaque_auth rq_cred;	/*  raw creds from the wire  */
	    caddr_t rq_clntcred;	/*  read only cooked cred  */
	    SVCXPRT *rq_xprt;		/*  associated transport  */
	};

	/*  if system supports this authenticate type indicated by rq_cred.oa_flavor,then
	 *  the struct pointed by rq_clntcred will be coverted by system to a new struct type
	 *  which is correspond with that authenticate type.
	 *  for unix authentication,rq_clntcred points to a struct named authunix_parms.
	 */

	/*  "raw" means that while RPC is running,system does not deal with the data pointed by
	 *  rq_cred.oa_base.
	 */

	<rpc/auth_unix.h>
	struct authunix_parms {
	    u_long aup_time;
	    char *aup_machname;
	    __uid_t aup_uid;
	    __gid_t aup_gid;
	    u_int aup_len;
	    __gid_t *aup_gids;
	};  /*  Unix style credentials  */

	<rpc/auth.h>
	struct opaque_auth {
	    enum_t oa_flavor;	/*  flavor of auth  */
	    caddr_t oa_base;	/*  address of more auth stuff  */
	    u_int oa_length;	/*  not to exceed MAX_AUTH_BYTES  */
	};

	<rpc/auth.h>
	AUTH *authunix_create_default(void);
	/*  authunix_create_default - create a default unix authentication.
	 *  return - AUTH pointer point to the new unix authentication.
	 */

	AUTH *authunix_create(char *host, int uid, int gid, int len, int *aup_gids);
	/*  authunix_create - create a unix authentication.
	 *  @host : host name.
	 *  @uid : used ID.
	 *  @gid : group ID.
	 *  @len : count of array aup_gids.
	 *  @aup_gids : aux groupID array.
	 *  return - return a pointer points to AUTH struct,return NULL if failed.
	 */

	 /*  authunix_create_default() use the suit parameters to calls authunix_create().  */
	 /*  struct CLENT has member cl_auth,it is type of AUTH * .  */

	void auth_destroy(AUTH *auth);
	/*  auth_destroy - destroy a AUTH struct.
	 *  @auth : a AUTH pointer points to the target.
	 */

	if svc_req.rq_cred.oa_flavor == AUTH_SYS,then server is able to knows user used unix authentication.
	/*  #define AUTH_NONE 0
	 *  #define AUTH_SYS 1
	 *  #define AUTH_UNIX AUTH_SYS
	 *  ...
	 */  /*  auth.h  */
	for get user identify informations,could covert rq_clntcred to struct authunix_parms *.

	Unix authentication is useless,because it is not secure,any user could create a RPC message with
	a customize unix authentication.
	(NFS use Unix authentication,but NFS request is often sent by kernel,and required a reserve port.
	 but root user still has a way to send customize NFS request.)

	RPC packet actualy has two fields associated with identify -
	  credential
	  verifier
	these fields combined a RPC identifier.
	/*  NULL authentication,two fields are null.
	 *  Unix authentication,verifier is null.
	 */

	other supported authentications :
	  AUTH_SHORT - another Unix authentication,size of AUTH_SHORT is less than AUTH_UNIX
	  AUTH_DES - DES is data encryption standard.this authenticate method is named secure RPC.
	  	     used on NFS is named secure NFS.
	  AUTH_KERB - based MIT Kerberos authenticating system.

    Timeout and Resend :
      Sun RPC has two timeout value :
        total timeout : the total time a client waiting for the server responds.(TCP,UDP)
	retry timeout : interval time for every resend request a client in the times waiting for server responds.(UDP)
	      	      	/*  TCP automatically resend  */

	<rpc/rpc.h>
	bool_t clnt_control(CLIENT *clnt, int req, char *info);
	/*  clnt_control - a macro used to change or retrive various information about a client object.
	 *  @clnt : client handle pointer.
	 *  @req : operation type.
	 *  @info : a pointer to the information,clnt_control will save info in it or retrive from it.
	 *  return - true or false.
	 */

	 /*  supported value of req :
	       CLSET_TIMEOUT		struct timeval
	       CLGET_TIMEOUT		struct timeval
	       CLGET_SERVER_ADDR	struct sockaddr_in
	       CLSET_RETRY_TIMEOUT	struct timeval
	       CLGET_RETRY_TIMEOUT	struct timeval

	     if use clnt_control() to set timeout,then the timeout parameter passed to clnt_call() will be
	     ignored in all future calls.
	     RETRY_TIMEOUT only effect UDP rpc connection.
	     the default timeout setted up by rpcgen is 25s.
	  */

      TCP connection manage :
        Sun RPC clnt default create a TCP connection for communicate with server.this TCP connection could be
	explicitly stopped by call clnt_destroy(),or implicitly stopped after process exited.

	<rpc/rpc.h>
	clnt_destroy(CLIENT *clnt);
	/*  clnt_destroy - a macro that destroys the client's RPC handle.
	    		   after destroyed,behavior of still use clnt is undefined,
			   if RPC library opened a socket,it will be closed after this
			   function was called,if the socket is not automatically opened
			   by RPC library,then it will still open.
	 */

      Transaction ID :
        Transaction ID,that is XID.Sun RPC use XID to identify a client request and server respond.
	while client send a RPC request,RPC system temporary provides a XID for this request,the
	respond by server must contains this XID as well.
	if this request is resent,XID still same.

	purposes of XID :
	  >  client use XID to identify respond from server.for TCP connection,it is rarely happens
	     different XID was received,but for UDP connection,it is contrary.
	  >  server caches responds they were sent to client,XID is used to help server determine
	     if the request is repeated.

      Server high-speed cache for repeated request :
        for active this feature,server have to call svc_dg_enablecache().
	Attention,if it is enabled,this feature will as longer as efficiency until server stop.

	<rpc/rpc.h>
	int svc_dg_enablecache(SVCXPRT *xprt, unsigned long size);
	/*  svc_dg_enablecache - enable server cache.
	 *    		       	 but on Linux 3.10,this function is not supported.
	 *  @xprt : transport handle,it is a member of struct svc_req.
	 *  @size : size of cahe should allocate.
	 *  return - 0 succeed,-1 failed.
	 */

	if enable server cache,then server holds a FIFO cache,every reply is identify by following
	fields :
	  program number
	  version number
	  procedure number
	  XID
	  client address(IP address and UDP port number)

    Invocation :
      classify invocation :
        1>  exactly once
	      procedure can only be executed just once.
	2>  at most once
	      procedure is either executed once or does not execute.
	3>  at least once
	      procedure at least execute once.
	      (for non-equipotential,this would make trouble.)
      
      (TCP)
      if received a reply,that means remote procedure was executed once,but if it's not,
      there is no way to know that is procedure was called or not.
      (before server crash or after server crush,network probably crush)

      (UDP,diable server cache)
      if received a reply and server is not crush,that means server procedure is at least
      called once.

      (UDP,enable server cache)
      if received a reply and server cache is enable,that means server procedure was called
      just once,constrary,at most once.

      these case means that :
        if there is no reason,dont use UDP to instead of TCP.
	for important non-equipotential remote procedure,should build a transaction system.
	for equipotential,UDP and disable server cache is trivial.
	for non-equipotential,UDP and disable server cache is a trouble.

    Prematurely terminate :
      prematurely terminate only has sence on TCP,because UDP dont care about drop packet
      and other cases.

      server prematurely terminate :
        when server stopped in a procedure invocation,TCP socket would be closed,and FIN section
	will be sent to client by OS.

	client is now blocked in clnt_call(),but FIN section was received,that is,unexpected reply
	has came,then error code will be recorded in CLIENT. 
	/*  all error code defined in <rpc/clnt.h>  */

	in multi-threads environment,call pthread_exit() to stopes worker but TCP socket is still
	opened,FIN section will not send to client.in this case,client will timeout.

      client prematurely terminate :
        client stopped when it is blocked in a procedure invocation before server reply,then OS
	send FIN section to server.
	
	server received FIN section(half_close, client no more data to sends),but server send
	result data,yet.

	client TCP responds RST for server message(OS operates),server will knows client is stopped
	at next	time to read or write on the TCP connection.

      <rpc/rpc.h>
      clnt_perror(CLIENT *clnt, char *s);
      char *clnt_sperror(CLIENT *clnt, char *s);
      /*  clnt_perror - print a message to stderr indicating why an RPC call failed.
       *  clnt_sperror - same as clnt_perror() but returns string to instead of write out to stderr.
       *  @clnt : client handle.
       *  @s : a string pointer,error message will prepended with @s and a colon.
       *  return - clnt_sperror() returns a static pointer points to error message.
       */

    XDR external data representation :
      XDR is kind of language used to describe data,and also a rule for coding.
      XDR use implicit typing to describing,this means peer is sending and peer is receiving
      have to know data type and byte order.
      all data type described via XDR needs n bytes,n = 4 * x,(x = 1, 2, 3, ...)
      and always sending follows big-endian.

      signed integers use two's complement.
      float-point use IEEE format.
      length varianous field at most has 3 bytes padding in the end.

      e.g.
        "hello" => XDR
	[5(4bytes), "hello", 0, 0, 0]
	(counter)   (src)   (padding)

      rpcgen transformat data defined in *.x to some special type in *.h
      /*
	const name = value;  =>  #define name value
        opaque var[n];  =>  char var[n];
	opaque var<m>;  =>  struct {
	       		        u_int var_len;
				char *var_val;
				} var;
	string var<m>;  =>  char *var;

	/*  @m can be ignored,but if it is specified at compile-time,then function libaray
	 *  would checks if the length is not greater than m.
	 */
       */

      <rpc/rpc.h>
      void xdrmem_create(XDR *xdrs, char *addr, unsigned int size, enum xdr_op op);
      /*  xdrmem_create - create a xdr memory stream used to decode or encode xdr information.
       *  @xdrs : XDR handler pointer.XDR handler is used to records XDR states.
       *  @addr : location memory space used as buffer.
       *  @size : size of buffer.
       *  @op : XDR operation,either XDR_DECODE or XDR_ENCODE or XDR_FREE.
       */
      /*  XDR handler struct XDR is defined in <rpc/xdr.h>  */

      unsigned int xdr_getpos(XDR *xdrs);
      /*  xdr_getpos - get current position in xdr memory stream.
       *  @xdrs : XDR handle.
       *  return - position offset.
       */

      xdr_setpos(XDR *xdrs, unsigned int pos);
      /*  xdr_setpos - a macro used to set position for xdr memory stream.
       *  @xdrs : XDR handle.
       *  @pos : new position.
       *  return - 1 on succeed,otherwise return 0.
       */
      /*  xdr_setpos() might works failed on one type of stream but works fine on another type.  */

      bool_t xdr_reference(XDR *xdrs, char *pp, unsigned int size, xdrproc_t proc);
      /*  xdr_reference - a primitive that provides pointer chasing within structures.
       *  @xdrs : XDR handle.
       *  @pp : a pointer points a pointer which points to the target structure.
       *  @size : size is the sizeof the structure that *pp points to.
       *  @proc : a function pointer of type of xdrproc.
       *	  a xdrproc_t exists for each data type which is to be encoded or decoded.
       */
      /*  xdr_reference() cant understand NULL pointer.  */
      /*  pointer chasing is the substitution of the pointer itself with the actual structure it
       *  points to.
       */
      /*  xdrproc is XDR routine that defined in <rpc/xdr.h>  */
      /*  rpcgen will creates other XDR routine for user-required data object which in *.x file  */

      RNDUP(x);
      /*  RNDUP - a macro it returns the nearest number near to x which is multiple of 4.
       *  @x : base value.
       */
      /*  BYTES_PER_XDR_UNIT is defined in <rpc/xdr.h>,RNDUP() uses this macro  */

     allocate suit buffer for XDR encode or decode,have to calculate the size of data object.
     for example :
       struct A {
         int a;
	 char b;
	 float c;
	 char d<5>;
       };
       A example;

       it at least has size : RNDUP(sizeof(example.a)) + RNDUP(sizeof(example.b)) + RNDUP(sizeof(example.c))
       	      	       	      + RNDUP(sizeof(int)) + RNDUP(sizeof(char) * 5)
       string has a counter which is type of int.
       
       /*  for maximum unspecified variable-length array,cant do such calculation "RNDUP(sizeof(char) * m)"  */

     optional data :
       XDR description has three methods to specify optional data.
         1>
	   union <name> switch (bool flag) {
	       case TRUE:
	           long val;
	       case FALSE:
	           void;
	   };

	   =>

	   struct <name> {
	       int flag;
	       union {
	           long val;
	       } <name>_u;
	   };

	   if flag is TRUE,a long type value is follow,otherwise,nothing is follow.
	   [4bytes flag = 1, value (long)]   	 (if flag is TRUE)
	   or
	   [4bytes flag = 0]			 (if flag is FALSE)

	 2>
	   struct <name> {
	       long arg<1>;
	   };

	   =>
	   
	   struct <name> {
	       struct {
	           u_int arg_len;
		   long *arg_val;
	       } arg;
	   }l

	   it has two format :
	   [4bytes arg_len = 1, value (long *)]  (if arg_len == 1)
	   or
	   [4bytes arg_len = 0]			 (if arg_len == 0)

	 3> 
	   struct <name> {
	       long *arg;
	   };

	   =>

	   struct <name> {
	       long *arg;
	   };

	   it has two format :
	   [4bytes flag = 1, value (long *)]	 (if not nullptr)
	   or
	   [4bytes flag = 0]			 (if nullptr)
	   /*  the flag on there is used to identify if pointer is NULL  */

     deal with list :
       in XDR description file :
         struct list_item {
	     string name<>;
	     long value;
	     list_item *next;
	 };

	 struct arg {
	     list_item *list;
	 };

	 =>

	 struct list_item {
	     char *name;	
	     long value;	
	     list_item *next;	
	 };

	 struct arg {
	     list_item *list;
	 };

	 string will encode to [strlen(string)bytes, string, (padding)]
	 long object will encode to [value (long)]
	 pointer will encode to [4bytes flag = 1, value (long *)] or [4bytes flag = 0]

	 at the time XDR to decodes list format data,it will automatically allocate memory
	 and build list,this is allow client easy to traverse list.

    RPC group form :
      RPC request in TCP section :
        IP head	     	 	 20B
	TCP head		 20B
	flag + length		 4B
	XID    			 4B		xid
	message type		 4B		msg_type (0 is call)
	RPC version		 4B		rpcvers
	program number		 4B		prog
	version number		 4B		vers
	procedure number	 4B		proc
	authentication		 4B		auth_flavor
	certificate length	 4B		
	certificate data	 <= 400B	
	authentication		 4B		auth_flavor
	verification length	 4B
	verify data  		 <= 400B
	procedure arguments
	
	    
	Sun RPC defined record,it could as request or replay.record is constructed by one or
	more than one fragment.fragment has a 4bytes(big-endian) value as head,top bit is the
	last fragment flag,lower 31bits is counter.if top bit is equal to zero,that means have
	other fragments	on there.

      RPC reply in UDP section :
        IP head			 20B		
	UDP head		 8B
	XID 			 4B		xid
	message type		 4B		msg_type (1 = replied)
	reply status		 4B		reply_stat (0 = accepted)
	authentication		 4B		auth_flavor
	verification length	 4B		
	verify data  		 <= 400B
	accept status		 4B		accept_stat (0 = succeed)
	procedure arguments

      /*  TPC is a stream,but UDP is a datagram,so UDP has length limit.some RPC implementation
      	  have been limited UDP length to <= 8192Bytes.
	  if want to send more data,have to use TCP.
       */

      /*  if certification == AUTH_SYS, verification should is AUTH_NONE  */

    Sun RPC provides total 164 functions :
      auth function (11)
      clnt function (26)
      pmap function (5)
      rpc function (24)
      svc function (44)
      xdr function (54)

	   
	   
	   
		
          



